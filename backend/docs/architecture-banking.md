# Architecture pour l'AgrÃ©gation Bancaire et Analyse IA

## Vue d'ensemble

Cette architecture est conÃ§ue pour :
1. **AgrÃ©ger les transactions** de multiples banques en temps rÃ©el
2. **Stocker** les donnÃ©es de maniÃ¨re fiable et scalable
3. **Analyser** les transactions via IA pour dÃ©tecter des patterns, catÃ©goriser, etc.

---

## ğŸ¯ Recommandation : Architecture Hybride Cloud SQL + BigQuery

### âœ… Pourquoi cette approche est judicieuse

**Cloud SQL (PostgreSQL)** pour :
- âœ… **OLTP** : Stockage transactionnel (crÃ©ation, lecture, mise Ã  jour)
- âœ… **DonnÃ©es opÃ©rationnelles** : comptes bancaires, connexions, mÃ©tadonnÃ©es
- âœ… **RequÃªtes complexes** avec relations (JOINs, transactions ACID)
- âœ… **Faible latence** pour les opÃ©rations CRUD
- âœ… **IntÃ©gration facile** avec votre stack Node.js/Express

**BigQuery** pour :
- âœ… **OLAP** : Analyse de grandes quantitÃ©s de donnÃ©es
- âœ… **Transactions historiques** : millions/billions de lignes
- âœ… **Analyses IA** : requÃªtes SQL complexes, ML intÃ©grÃ©
- âœ… **ScalabilitÃ© automatique** : pas de gestion de capacitÃ©
- âœ… **CoÃ»t optimisÃ©** : pay-per-query, pas de serveur Ã  maintenir

---

## ğŸ—ï¸ Architecture RecommandÃ©e

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        FRONTEND                              â”‚
â”‚                    (React + Firebase Auth)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BACKEND API                               â”‚
â”‚              (Node.js/Express sur Cloud Run)                 â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  Auth        â”‚  â”‚  Banking     â”‚  â”‚  Analytics   â”‚      â”‚
â”‚  â”‚  Routes      â”‚  â”‚  Routes      â”‚  â”‚  Routes      â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                  â”‚                   â”‚
        â”‚                  â”‚                   â”‚
        â–¼                  â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Firebase    â”‚  â”‚  Cloud SQL   â”‚  â”‚  BigQuery    â”‚
â”‚  Auth        â”‚  â”‚  (Postgres)  â”‚  â”‚  (Data Lake) â”‚
â”‚              â”‚  â”‚              â”‚  â”‚              â”‚
â”‚  - Users     â”‚  â”‚  - Accounts  â”‚  â”‚  - Transactionsâ”‚
â”‚  - Tenants   â”‚  â”‚  - Connectionsâ”‚  â”‚  - Analytics â”‚
â”‚              â”‚  â”‚  - Metadata  â”‚  â”‚  - ML Models â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚                   â”‚
                        â”‚                   â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚  Data Pipeline   â”‚
                        â”‚  (Cloud Functionsâ”‚
                        â”‚   ou Pub/Sub)    â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚  Banking APIs    â”‚
                        â”‚  (Tink, Plaid,   â”‚
                        â”‚   Yodlee, etc.)  â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š ModÃ¨le de DonnÃ©es

### Cloud SQL (PostgreSQL) - DonnÃ©es OpÃ©rationnelles

```sql
-- Connexions bancaires
CREATE TABLE bank_connections (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    tenant_id UUID NOT NULL,
    user_id UUID NOT NULL,
    provider VARCHAR(50) NOT NULL, -- 'tink', 'plaid', 'yodlee'
    provider_account_id VARCHAR(255) NOT NULL,
    access_token_encrypted TEXT, -- ChiffrÃ© via Secret Manager
    refresh_token_encrypted TEXT,
    status VARCHAR(20) DEFAULT 'active', -- 'active', 'expired', 'revoked'
    last_sync_at TIMESTAMP,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW(),
    UNIQUE(tenant_id, provider, provider_account_id)
);

-- Comptes bancaires
CREATE TABLE bank_accounts (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    connection_id UUID REFERENCES bank_connections(id) ON DELETE CASCADE,
    provider_account_id VARCHAR(255) NOT NULL,
    account_type VARCHAR(50), -- 'checking', 'savings', 'credit'
    account_name VARCHAR(255),
    balance DECIMAL(15,2),
    currency VARCHAR(3) DEFAULT 'EUR',
    last_balance_update TIMESTAMP,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

-- Index pour performances
CREATE INDEX idx_connections_tenant ON bank_connections(tenant_id);
CREATE INDEX idx_connections_user ON bank_connections(user_id);
CREATE INDEX idx_accounts_connection ON bank_accounts(connection_id);
```

### BigQuery - Transactions et Analytics

```sql
-- Table des transactions (partitionnÃ©e par date)
CREATE TABLE `project.dataset.transactions` (
    transaction_id STRING,
    tenant_id STRING,
    user_id STRING,
    connection_id STRING,
    account_id STRING,
    provider STRING,
    provider_transaction_id STRING,
    
    -- DonnÃ©es transaction
    amount DECIMAL(15,2),
    currency STRING,
    date DATE,
    description STRING,
    category STRING, -- CatÃ©gorie brute de la banque
    merchant_name STRING,
    
    -- MÃ©tadonnÃ©es
    transaction_type STRING, -- 'debit', 'credit', 'transfer'
    status STRING, -- 'pending', 'completed', 'cancelled'
    
    -- Enrichissement IA (rempli par pipeline)
    ai_category STRING, -- CatÃ©gorie enrichie par IA
    ai_tags ARRAY<STRING>, -- Tags gÃ©nÃ©rÃ©s par IA
    ai_confidence FLOAT64,
    ai_notes STRING, -- Notes gÃ©nÃ©rÃ©es par IA
    
    -- Timestamps
    created_at TIMESTAMP,
    updated_at TIMESTAMP,
    ingested_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP()
)
PARTITION BY date
CLUSTER BY tenant_id, user_id;

-- Index pour requÃªtes frÃ©quentes
CREATE INDEX idx_transactions_tenant_date 
ON `project.dataset.transactions`(tenant_id, date);
```

---

## ğŸ”„ Flux de DonnÃ©es

### 1. Connexion Bancaire (Initial Setup)

```
User â†’ Frontend â†’ Backend API â†’ Banking Provider (Tink)
  â†“
Backend reÃ§oit access_token
  â†“
Backend chiffre et stocke dans Cloud SQL (bank_connections)
  â†“
Backend rÃ©cupÃ¨re la liste des comptes
  â†“
Backend stocke les comptes dans Cloud SQL (bank_accounts)
```

### 2. Synchronisation des Transactions (Temps RÃ©el)

**Option A : Webhooks (RecommandÃ©)**
```
Banking Provider â†’ Webhook â†’ Cloud Run Endpoint
  â†“
Backend valide et traite les transactions
  â†“
Backend Ã©crit dans Cloud SQL (pour validation/audit)
  â†“
Cloud Function/Pub-Sub dÃ©clenche le pipeline
  â†“
Pipeline Ã©crit dans BigQuery (pour analytics)
  â†“
Pipeline dÃ©clenche l'analyse IA (Vertex AI)
  â†“
RÃ©sultats IA mis Ã  jour dans BigQuery
```

**Option B : Polling (Fallback)**
```
Cloud Scheduler â†’ Cloud Function (toutes les heures)
  â†“
Cloud Function appelle Banking Provider API
  â†“
Nouvelles transactions dÃ©tectÃ©es
  â†“
MÃªme pipeline que Option A
```

### 3. Analyse IA des Transactions

```
BigQuery â†’ Vertex AI (ou API externe)
  â†“
IA analyse la transaction (description, montant, contexte)
  â†“
IA gÃ©nÃ¨re :
  - CatÃ©gorie enrichie
  - Tags (ex: "abonnement", "restaurant", "transport")
  - Notes (ex: "Abonnement Netflix dÃ©tectÃ©")
  - Score de confiance
  â†“
RÃ©sultats Ã©crits dans BigQuery (colonnes ai_*)
```

---

## ğŸ› ï¸ ImplÃ©mentation Technique

### Backend - Routes Banking

```javascript
// src/routes/banking.js

import { Router } from 'express';
import { Pool } from 'pg';
import { SecretManagerServiceClient } from '@google-cloud/secret-manager';

const router = Router();
const pool = new Pool({
  // Connection config depuis env vars
});

// Connecter un compte bancaire
router.post('/connections', requireAuth, async (req, res) => {
  const { provider, accessToken, refreshToken } = req.body;
  
  // 1. Chiffrer les tokens
  const secretClient = new SecretManagerServiceClient();
  const encryptedToken = await encryptToken(accessToken, secretClient);
  
  // 2. Stocker dans Cloud SQL
  const result = await pool.query(`
    INSERT INTO bank_connections 
    (tenant_id, user_id, provider, access_token_encrypted, ...)
    VALUES ($1, $2, $3, $4, ...)
    RETURNING id
  `, [req.user.tenantId, req.user.uid, provider, encryptedToken]);
  
  // 3. RÃ©cupÃ©rer les comptes depuis le provider
  const accounts = await fetchAccountsFromProvider(provider, accessToken);
  
  // 4. Stocker les comptes
  await storeAccounts(result.rows[0].id, accounts);
  
  res.json({ connectionId: result.rows[0].id, accounts });
});

// Webhook pour recevoir les transactions
router.post('/webhooks/:provider', async (req, res) => {
  const { provider } = req.params;
  const transactions = req.body.transactions;
  
  // 1. Valider la signature du webhook
  if (!validateWebhookSignature(provider, req)) {
    return res.status(401).json({ error: 'Invalid signature' });
  }
  
  // 2. Traiter chaque transaction
  for (const tx of transactions) {
    // Ã‰crire dans Cloud SQL (audit)
    await pool.query(`
      INSERT INTO transaction_logs (provider, transaction_id, raw_data)
      VALUES ($1, $2, $3)
    `, [provider, tx.id, JSON.stringify(tx)]);
    
    // Publier dans Pub/Sub pour traitement asynchrone
    await pubsub.topic('transactions').publishMessage({
      json: {
        tenantId: tx.tenantId,
        transaction: tx
      }
    });
  }
  
  res.json({ received: transactions.length });
});
```

### Pipeline de DonnÃ©es (Cloud Function)

```javascript
// functions/process-transaction/index.js

const { BigQuery } = require('@google-cloud/bigquery');
const { VertexAI } = require('@google-cloud/vertexai');

const bigquery = new BigQuery();
const vertexAI = new VertexAI({ project: 'your-project' });

exports.processTransaction = async (pubsubMessage, context) => {
  const { tenantId, transaction } = JSON.parse(
    Buffer.from(pubsubMessage.data, 'base64').toString()
  );
  
  // 1. InsÃ©rer dans BigQuery
  await bigquery
    .dataset('banking')
    .table('transactions')
    .insert([{
      transaction_id: transaction.id,
      tenant_id: tenantId,
      amount: transaction.amount,
      description: transaction.description,
      date: transaction.date,
      // ... autres champs
    }]);
  
  // 2. Analyser via IA
  const aiAnalysis = await analyzeWithAI(transaction);
  
  // 3. Mettre Ã  jour avec les rÃ©sultats IA
  await bigquery
    .dataset('banking')
    .table('transactions')
    .update({
      ai_category: aiAnalysis.category,
      ai_tags: aiAnalysis.tags,
      ai_confidence: aiAnalysis.confidence,
      ai_notes: aiAnalysis.notes,
    })
    .where(`transaction_id = '${transaction.id}'`);
};

async function analyzeWithAI(transaction) {
  const model = vertexAI.getGenerativeModel({
    model: 'gemini-pro',
  });
  
  const prompt = `
    Analyse cette transaction bancaire et fournis :
    - CatÃ©gorie enrichie (ex: "Abonnement", "Restaurant", "Transport")
    - Tags pertinents
    - Notes explicatives
    - Score de confiance (0-1)
    
    Transaction:
    - Description: ${transaction.description}
    - Montant: ${transaction.amount} ${transaction.currency}
    - Date: ${transaction.date}
  `;
  
  const result = await model.generateContent(prompt);
  // Parser la rÃ©ponse et retourner l'objet structurÃ©
  return parseAIResponse(result);
}
```

---

## ğŸ” SÃ©curitÃ©

1. **Tokens bancaires** : Chiffrement via Google Secret Manager
2. **Webhooks** : Validation de signature HMAC
3. **Isolation des donnÃ©es** : Row-level security dans BigQuery (par tenant_id)
4. **Audit** : Toutes les opÃ©rations loggÃ©es dans Cloud Logging

---

## ğŸ’° CoÃ»ts EstimÃ©s

### Cloud SQL (PostgreSQL)
- **Instance** : ~$50-200/mois (selon taille)
- **Stockage** : ~$0.17/GB/mois
- **Backups** : ~$0.08/GB/mois

### BigQuery
- **Stockage** : ~$0.02/GB/mois (premiers 10GB gratuits)
- **RequÃªtes** : ~$5/TB scannÃ© (premiers 1TB/mois gratuits)
- **Streaming inserts** : ~$0.012/200MB

### Vertex AI
- **Gemini Pro** : ~$0.0005/1K tokens (input), ~$0.0015/1K tokens (output)

**Estimation pour 10K utilisateurs, 1M transactions/mois** :
- Cloud SQL : ~$100-150/mois
- BigQuery : ~$50-100/mois
- Vertex AI : ~$20-50/mois
- **Total** : ~$170-300/mois

---

## ğŸ” Pourquoi BigQuery n'est PAS adaptÃ© pour OLTP ?

### DiffÃ©rence fondamentale : OLTP vs OLAP

**OLTP (Online Transaction Processing)** - Cloud SQL :
- âœ… **Latence ultra-faible** : < 10ms pour une requÃªte simple
- âœ… **Transactions ACID** : garanties de cohÃ©rence immÃ©diate
- âœ… **Concurrence Ã©levÃ©e** : milliers de requÃªtes simultanÃ©es
- âœ… **OpÃ©rations atomiques** : INSERT/UPDATE/DELETE individuels
- âœ… **Index optimisÃ©s** : B-tree pour accÃ¨s direct par clÃ©

**OLAP (Online Analytical Processing)** - BigQuery :
- âœ… **Analyses massives** : scan de millions/billions de lignes
- âœ… **RequÃªtes complexes** : agrÃ©gations, JOINs sur grandes tables
- âœ… **ParallÃ©lisation** : distribue le travail sur des milliers de machines
- âŒ **Latence Ã©levÃ©e** : 100ms-30s selon la complexitÃ©
- âŒ **Pas de transactions ACID** : pas de garanties de cohÃ©rence immÃ©diate

### ProblÃ¨mes concrets avec BigQuery pour OLTP

#### 1. **Latence trop Ã©levÃ©e**

```javascript
// âŒ BigQuery : 200-500ms minimum
const result = await bigquery.query(`
  SELECT * FROM transactions 
  WHERE transaction_id = 'tx_123'
`);
// Latence : 200-500ms (mÃªme pour 1 ligne!)

// âœ… Cloud SQL : < 10ms
const result = await pool.query(`
  SELECT * FROM transactions 
  WHERE transaction_id = $1
`, ['tx_123']);
// Latence : 2-10ms
```

**Impact** : Une page web qui fait 10 requÃªtes = 2-5 secondes vs 20-100ms

#### 2. **Pas de transactions ACID**

```javascript
// âŒ BigQuery : Pas de transactions multi-statements
// Impossible de faire :
BEGIN;
  INSERT INTO accounts (id, balance) VALUES ('acc_1', 1000);
  INSERT INTO transactions (account_id, amount) VALUES ('acc_1', -50);
  UPDATE accounts SET balance = 950 WHERE id = 'acc_1';
COMMIT;
// Si une opÃ©ration Ã©choue, pas de rollback automatique!

// âœ… Cloud SQL : Transactions ACID garanties
await pool.query('BEGIN');
try {
  await pool.query('INSERT INTO accounts ...');
  await pool.query('INSERT INTO transactions ...');
  await pool.query('UPDATE accounts ...');
  await pool.query('COMMIT');
} catch (err) {
  await pool.query('ROLLBACK');
}
```

**Impact** : Risque d'incohÃ©rence des donnÃ©es (ex: transaction enregistrÃ©e mais balance non mise Ã  jour)

#### 3. **CoÃ»t prohibitif pour les requÃªtes frÃ©quentes**

```javascript
// Exemple : VÃ©rifier si un compte existe (1000 fois/seconde)

// âŒ BigQuery : 
// - Scan minimum : 10MB (mÃªme pour 1 ligne)
// - CoÃ»t : $5/TB scannÃ©
// - 1000 req/s Ã— 10MB = 10GB/s = 864TB/jour
// - CoÃ»t : 864TB Ã— $5 = $4,320/jour = $129,600/mois ğŸ˜±

// âœ… Cloud SQL :
// - Index B-tree : accÃ¨s direct
// - Scan : quelques KB
// - CoÃ»t : fixe (~$100/mois pour l'instance)
```

#### 4. **Pas d'index optimisÃ©s pour les lookups**

```sql
-- âŒ BigQuery : Scan complet de la partition
SELECT * FROM transactions 
WHERE user_id = 'user_123' 
  AND date = '2024-01-15'
  AND transaction_id = 'tx_456';
-- BigQuery doit scanner TOUTE la partition de la date
-- MÃªme avec clustering, c'est beaucoup plus lent qu'un index B-tree

-- âœ… Cloud SQL : Index composite = accÃ¨s direct
CREATE INDEX idx_user_date_tx 
ON transactions(user_id, date, transaction_id);
-- AccÃ¨s direct en O(log n) via B-tree
```

#### 5. **Limites de concurrence**

```javascript
// âŒ BigQuery :
// - Limite : ~100 requÃªtes simultanÃ©es par projet
// - Queue si dÃ©passement
// - Pas adaptÃ© pour une API avec milliers d'utilisateurs simultanÃ©s

// âœ… Cloud SQL :
// - Milliers de connexions simultanÃ©es
// - Connection pooling efficace
// - Pas de queue pour les requÃªtes simples
```

#### 6. **Pas de contraintes de clÃ© Ã©trangÃ¨re en temps rÃ©el**

```sql
-- âŒ BigQuery : Pas de validation immÃ©diate
INSERT INTO transactions (account_id, ...) 
VALUES ('invalid_account', ...);
-- Pas d'erreur immÃ©diate, problÃ¨me dÃ©couvert plus tard

-- âœ… Cloud SQL : Validation immÃ©diate
ALTER TABLE transactions 
ADD CONSTRAINT fk_account 
FOREIGN KEY (account_id) REFERENCES accounts(id);
-- Erreur immÃ©diate si account_id n'existe pas
```

### Comparaison concrÃ¨te : Cas d'usage typique

**ScÃ©nario** : Un utilisateur ouvre son dashboard et charge ses 50 derniÃ¨res transactions

```javascript
// âŒ Avec BigQuery uniquement
async function getUserTransactions(userId) {
  // 1. VÃ©rifier que l'utilisateur existe
  const user = await bigquery.query(`
    SELECT * FROM users WHERE id = '${userId}'
  `); // 200ms
  
  // 2. RÃ©cupÃ©rer les comptes
  const accounts = await bigquery.query(`
    SELECT * FROM accounts WHERE user_id = '${userId}'
  `); // 200ms
  
  // 3. RÃ©cupÃ©rer les transactions
  const transactions = await bigquery.query(`
    SELECT * FROM transactions 
    WHERE user_id = '${userId}'
    ORDER BY date DESC LIMIT 50
  `); // 500ms (scan de la partition)
  
  // Total : ~900ms + coÃ»t Ã©levÃ©
}

// âœ… Avec Cloud SQL + BigQuery (architecture hybride)
async function getUserTransactions(userId) {
  // 1. VÃ©rifier que l'utilisateur existe (Cloud SQL)
  const user = await pool.query(
    'SELECT * FROM users WHERE id = $1', [userId]
  ); // 2ms
  
  // 2. RÃ©cupÃ©rer les comptes (Cloud SQL)
  const accounts = await pool.query(
    'SELECT * FROM accounts WHERE user_id = $1', [userId]
  ); // 3ms
  
  // 3. RÃ©cupÃ©rer les transactions rÃ©centes (Cloud SQL pour les 30 derniers jours)
  const recentTx = await pool.query(`
    SELECT * FROM transactions 
    WHERE user_id = $1 AND date >= NOW() - INTERVAL '30 days'
    ORDER BY date DESC LIMIT 50
  `, [userId]); // 5ms
  
  // 4. Si besoin d'analytics historiques, utiliser BigQuery
  const analytics = await bigquery.query(`
    SELECT category, SUM(amount) as total
    FROM transactions 
    WHERE user_id = '${userId}' AND date >= '2024-01-01'
    GROUP BY category
  `); // 500ms (mais seulement si nÃ©cessaire)
  
  // Total : ~10ms pour les donnÃ©es opÃ©rationnelles
  // + 500ms seulement si analytics nÃ©cessaires
}
```

### Conclusion

**BigQuery est excellent pour** :
- âœ… Analyser des millions de transactions
- âœ… RequÃªtes complexes avec agrÃ©gations
- âœ… Data warehousing et analytics
- âœ… ML et analyses IA

**BigQuery est mauvais pour** :
- âŒ RequÃªtes frÃ©quentes par clÃ© (lookups)
- âŒ Transactions ACID
- âŒ OpÃ©rations CRUD individuelles
- âŒ Applications avec faible latence requise

**C'est pourquoi l'architecture hybride Cloud SQL + BigQuery est optimale** :
- Cloud SQL pour les opÃ©rations transactionnelles (rapide, ACID, peu coÃ»teux)
- BigQuery pour les analyses et l'historique (scalable, puissant, optimisÃ© pour l'analytique)

---

## ğŸš€ Alternatives Ã  ConsidÃ©rer

### Alternative 1 : Tout dans BigQuery
- âœ… Plus simple (une seule base)
- âŒ Moins performant pour les requÃªtes OLTP (voir section ci-dessus)
- âŒ CoÃ»t plus Ã©levÃ© pour les opÃ©rations frÃ©quentes
- âŒ Latence trop Ã©levÃ©e pour une API web
- **Verdict** : Pas recommandÃ© pour les opÃ©rations transactionnelles

### Alternative 2 : Cloud SQL + Firestore
- âœ… Firestore pour les mÃ©tadonnÃ©es (plus flexible)
- âœ… Cloud SQL pour les transactions
- âŒ Plus complexe Ã  maintenir
- **Verdict** : Possible si vous avez dÃ©jÃ  Firestore, mais pas nÃ©cessaire

### Alternative 3 : AlloyDB (PostgreSQL compatible)
- âœ… Meilleures performances que Cloud SQL
- âœ… IntÃ©gration native avec BigQuery
- âŒ Plus cher
- **Verdict** : Ã€ considÃ©rer si vous avez beaucoup de charge

### Alternative 4 : Spanner (Global, multi-rÃ©gion)
- âœ… Ultra-scalable, global
- âŒ Beaucoup plus cher
- âŒ Overkill pour un MVP
- **Verdict** : Pour plus tard, si vous scalez internationalement

---

## ğŸ“‹ Checklist d'ImplÃ©mentation

### Phase 1 : MVP
- [ ] Setup Cloud SQL (PostgreSQL)
- [ ] Setup BigQuery dataset
- [ ] ImplÃ©menter les routes de connexion bancaire
- [ ] ImplÃ©menter le webhook de rÃ©ception
- [ ] Pipeline basique Cloud SQL â†’ BigQuery
- [ ] Interface frontend pour visualiser les transactions

### Phase 2 : Analyse IA
- [ ] IntÃ©gration Vertex AI (ou API externe)
- [ ] Pipeline d'enrichissement IA
- [ ] Mise Ã  jour des transactions avec rÃ©sultats IA
- [ ] Interface pour visualiser les catÃ©gories/tags IA

### Phase 3 : Optimisation
- [ ] Cache Redis pour requÃªtes frÃ©quentes
- [ ] Indexation optimale BigQuery
- [ ] Monitoring et alertes
- [ ] Dashboard analytics

---

## ğŸ”„ AgrÃ©gation Multi-Sources et Croisement de DonnÃ©es

### Cas d'usage : Plusieurs sources avec formats diffÃ©rents

**ScÃ©nario** :
- **Source A & B** â†’ Table `transactions_unified` (2 sources agrÃ©gÃ©es)
- **Source C** â†’ Table `external_data` (1 source sÃ©parÃ©e)
- **Besoin** : Croiser les donnÃ©es entre tables pour analyse IA
- **Contrainte** : Conservation 10 ans sur Cloud SQL

### Architecture RecommandÃ©e

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Sources de DonnÃ©es                       â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  Source A    â”‚  â”‚  Source B    â”‚  â”‚  Source C    â”‚     â”‚
â”‚  â”‚  (Format 1)  â”‚  â”‚  (Format 2)  â”‚  â”‚  (Format 3)  â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚         â”‚                 â”‚                 â”‚              â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚              â”‚
â”‚                  â”‚                          â”‚              â”‚
â”‚                  â–¼                          â–¼              â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚         â”‚  ETL Pipeline   â”‚      â”‚  ETL Pipeline   â”‚      â”‚
â”‚         â”‚  (Normalisation)â”‚      â”‚  (Normalisation)â”‚      â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                  â”‚                          â”‚               â”‚
â”‚                  â–¼                          â–¼               â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚    â”‚  Cloud SQL              â”‚  â”‚  Cloud SQL       â”‚      â”‚
â”‚    â”‚  transactions_unified   â”‚  â”‚  external_data   â”‚      â”‚
â”‚    â”‚  (Source A + B)         â”‚  â”‚  (Source C)      â”‚      â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                 â”‚                         â”‚                â”‚
â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                            â”‚                                â”‚
â”‚                            â–¼                                â”‚
â”‚                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚                 â”‚  Data Pipeline        â”‚                   â”‚
â”‚                 â”‚  (Cloud SQL â†’ BQ)     â”‚                   â”‚
â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                            â”‚                                â”‚
â”‚                            â–¼                                â”‚
â”‚                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚                 â”‚  BigQuery             â”‚                   â”‚
â”‚                 â”‚  - transactions_unified                   â”‚
â”‚                 â”‚  - external_data                          â”‚
â”‚                 â”‚  - joined_analytics (vue)                 â”‚
â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                            â”‚                                â”‚
â”‚                            â–¼                                â”‚
â”‚                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚                 â”‚  Vertex AI            â”‚                   â”‚
â”‚                 â”‚  (Analyse croisÃ©e)    â”‚                   â”‚
â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1. SchÃ©ma de Normalisation (Cloud SQL)

#### Table unifiÃ©e pour Sources A & B

```sql
-- Table principale pour Sources A & B (format unifiÃ©)
CREATE TABLE transactions_unified (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    tenant_id UUID NOT NULL,
    user_id UUID NOT NULL,
    
    -- Identifiants source
    source_type VARCHAR(50) NOT NULL, -- 'source_a', 'source_b'
    source_id VARCHAR(255) NOT NULL, -- ID original de la source
    source_raw_data JSONB, -- DonnÃ©es brutes pour traÃ§abilitÃ©
    
    -- Champs normalisÃ©s (commun aux 2 sources)
    amount DECIMAL(15,2) NOT NULL,
    currency VARCHAR(3) DEFAULT 'EUR',
    transaction_date DATE NOT NULL,
    transaction_time TIMESTAMP,
    description TEXT,
    merchant_name VARCHAR(255),
    
    -- CatÃ©gorisation
    category VARCHAR(100),
    subcategory VARCHAR(100),
    
    -- MÃ©tadonnÃ©es
    status VARCHAR(20) DEFAULT 'completed', -- 'pending', 'completed', 'cancelled'
    transaction_type VARCHAR(20), -- 'debit', 'credit', 'transfer'
    
    -- Champs optionnels (peuvent Ãªtre NULL selon la source)
    reference_number VARCHAR(255),
    iban VARCHAR(34),
    account_number VARCHAR(50),
    balance_after DECIMAL(15,2),
    
    -- TraÃ§abilitÃ©
    ingested_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW(),
    source_metadata JSONB, -- MÃ©tadonnÃ©es spÃ©cifiques Ã  la source
    
    -- Index pour performances
    CONSTRAINT unique_source_transaction UNIQUE(source_type, source_id),
    CONSTRAINT check_source_type CHECK (source_type IN ('source_a', 'source_b'))
);

-- Index pour requÃªtes frÃ©quentes
CREATE INDEX idx_transactions_tenant_date ON transactions_unified(tenant_id, transaction_date);
CREATE INDEX idx_transactions_user_date ON transactions_unified(user_id, transaction_date);
CREATE INDEX idx_transactions_source ON transactions_unified(source_type, source_id);
CREATE INDEX idx_transactions_ingested ON transactions_unified(ingested_at);

-- Index GIN pour recherche dans JSONB
CREATE INDEX idx_transactions_raw_data ON transactions_unified USING GIN(source_raw_data);
```

#### Table sÃ©parÃ©e pour Source C

```sql
-- Table pour Source C (structure diffÃ©rente)
CREATE TABLE external_data (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    tenant_id UUID NOT NULL,
    user_id UUID NOT NULL,
    
    -- Identifiants source
    source_type VARCHAR(50) DEFAULT 'source_c',
    source_id VARCHAR(255) NOT NULL,
    source_raw_data JSONB,
    
    -- Champs spÃ©cifiques Ã  Source C
    event_type VARCHAR(100) NOT NULL, -- DiffÃ©rent de "transaction"
    event_date DATE NOT NULL,
    event_timestamp TIMESTAMP,
    
    -- DonnÃ©es spÃ©cifiques (structure flexible)
    data_fields JSONB NOT NULL, -- Structure libre selon le type d'Ã©vÃ©nement
    
    -- Champs communs pour croisement
    amount DECIMAL(15,2), -- Peut Ãªtre NULL si pas applicable
    currency VARCHAR(3),
    description TEXT,
    
    -- MÃ©tadonnÃ©es
    status VARCHAR(20) DEFAULT 'active',
    priority INTEGER DEFAULT 0,
    
    -- TraÃ§abilitÃ©
    ingested_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW(),
    
    CONSTRAINT unique_source_c_event UNIQUE(source_type, source_id)
);

-- Index pour croisement avec transactions_unified
CREATE INDEX idx_external_tenant_date ON external_data(tenant_id, event_date);
CREATE INDEX idx_external_user_date ON external_data(user_id, event_date);
CREATE INDEX idx_external_event_type ON external_data(event_type);
CREATE INDEX idx_external_data_fields ON external_data USING GIN(data_fields);
```

#### Table de mapping pour croisement

```sql
-- Table pour mapper les relations entre sources
CREATE TABLE data_relationships (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    tenant_id UUID NOT NULL,
    
    -- Relation entre transactions_unified et external_data
    transaction_id UUID REFERENCES transactions_unified(id) ON DELETE CASCADE,
    external_data_id UUID REFERENCES external_data(id) ON DELETE CASCADE,
    
    -- Type de relation
    relationship_type VARCHAR(50) NOT NULL, -- 'same_event', 'related', 'duplicate', 'ai_matched'
    confidence_score FLOAT DEFAULT 1.0, -- 0.0 Ã  1.0
    
    -- CritÃ¨res de matching
    match_criteria JSONB, -- Ex: {'date_diff': '1 day', 'amount_diff': 0.01}
    
    -- MÃ©tadonnÃ©es
    created_by VARCHAR(50) DEFAULT 'system', -- 'system', 'ai', 'manual'
    created_at TIMESTAMP DEFAULT NOW(),
    
    CONSTRAINT unique_relationship UNIQUE(transaction_id, external_data_id, relationship_type)
);

CREATE INDEX idx_relationships_transaction ON data_relationships(transaction_id);
CREATE INDEX idx_relationships_external ON data_relationships(external_data_id);
CREATE INDEX idx_relationships_tenant ON data_relationships(tenant_id);
```

### 2. ETL Pipeline : Normalisation Multi-Sources

```javascript
// src/services/etl/normalizer.js

/**
 * Normalise les donnÃ©es de Source A vers le format unifiÃ©
 */
export function normalizeSourceA(rawData) {
  return {
    source_type: 'source_a',
    source_id: rawData.transactionId || rawData.id,
    source_raw_data: rawData, // Conserver les donnÃ©es brutes
    
    // Mapping des champs
    amount: parseFloat(rawData.amount || rawData.value),
    currency: rawData.currency || 'EUR',
    transaction_date: new Date(rawData.date || rawData.timestamp).toISOString().split('T')[0],
    transaction_time: new Date(rawData.timestamp || rawData.date),
    description: rawData.description || rawData.label || rawData.narrative,
    merchant_name: rawData.merchant || rawData.counterparty?.name,
    category: rawData.category || rawData.type,
    subcategory: rawData.subcategory,
    status: mapStatusSourceA(rawData.status),
    transaction_type: mapTransactionTypeSourceA(rawData.direction),
    
    // Champs optionnels
    reference_number: rawData.reference,
    iban: rawData.counterparty?.iban,
    account_number: rawData.account?.number,
    balance_after: rawData.balance ? parseFloat(rawData.balance) : null,
    
    source_metadata: {
      original_format: 'source_a',
      api_version: rawData.apiVersion,
      // Autres mÃ©tadonnÃ©es spÃ©cifiques
    }
  };
}

/**
 * Normalise les donnÃ©es de Source B vers le format unifiÃ©
 */
export function normalizeSourceB(rawData) {
  return {
    source_type: 'source_b',
    source_id: rawData.tx_id || rawData.transaction_id,
    source_raw_data: rawData,
    
    // Mapping diffÃ©rent de Source A
    amount: parseFloat(rawData.montant || rawData.amount),
    currency: rawData.devise || rawData.currency || 'EUR',
    transaction_date: parseDateSourceB(rawData.date_operation || rawData.date),
    transaction_time: parseTimestampSourceB(rawData.date_operation || rawData.date),
    description: rawData.libelle || rawData.description || rawData.detail,
    merchant_name: rawData.commercant || rawData.merchant,
    category: rawData.categorie || rawData.category,
    subcategory: rawData.sous_categorie || rawData.subcategory,
    status: mapStatusSourceB(rawData.statut || rawData.status),
    transaction_type: mapTransactionTypeSourceB(rawData.sens || rawData.type),
    
    reference_number: rawData.numero_operation || rawData.operation_number,
    iban: rawData.iban_compte || rawData.account_iban,
    account_number: rawData.numero_compte || rawData.account_number,
    balance_after: rawData.solde_apres ? parseFloat(rawData.solde_apres) : null,
    
    source_metadata: {
      original_format: 'source_b',
      bank_code: rawData.code_banque,
      // Autres mÃ©tadonnÃ©es spÃ©cifiques
    }
  };
}

/**
 * Normalise les donnÃ©es de Source C (structure diffÃ©rente)
 */
export function normalizeSourceC(rawData) {
  return {
    source_type: 'source_c',
    source_id: rawData.eventId || rawData.id,
    source_raw_data: rawData,
    
    event_type: rawData.type || rawData.eventType,
    event_date: new Date(rawData.date || rawData.eventDate).toISOString().split('T')[0],
    event_timestamp: new Date(rawData.timestamp || rawData.eventDate),
    
    // DonnÃ©es flexibles en JSONB
    data_fields: {
      // Structure libre selon event_type
      ...rawData,
      // Exemples selon le type :
      // Si event_type = 'invoice': { invoice_number, due_date, vendor }
      // Si event_type = 'payment': { payment_method, recipient }
      // etc.
    },
    
    // Champs optionnels pour croisement
    amount: rawData.amount ? parseFloat(rawData.amount) : null,
    currency: rawData.currency || 'EUR',
    description: rawData.description || rawData.summary,
    
    status: rawData.status || 'active',
    priority: rawData.priority || 0,
  };
}

// Routes d'ingestion
// src/routes/data-ingestion.js

import { Router } from 'express';
import { Pool } from 'pg';
import { normalizeSourceA, normalizeSourceB, normalizeSourceC } from '../services/etl/normalizer.js';
import { publishToPubSub } from '../services/pubsub.js';

const router = Router();
const pool = new Pool({ /* config */ });

// Ingestion Source A
router.post('/ingest/source-a', requireAuth, async (req, res) => {
  const { transactions } = req.body;
  const normalized = transactions.map(normalizeSourceA);
  
  // Insert dans Cloud SQL
  for (const tx of normalized) {
    await pool.query(`
      INSERT INTO transactions_unified (
        tenant_id, user_id, source_type, source_id, source_raw_data,
        amount, currency, transaction_date, transaction_time, description,
        merchant_name, category, status, transaction_type, source_metadata
      ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15)
      ON CONFLICT (source_type, source_id) 
      DO UPDATE SET 
        amount = EXCLUDED.amount,
        updated_at = NOW()
    `, [
      req.user.tenantId, req.user.uid,
      tx.source_type, tx.source_id, JSON.stringify(tx.source_raw_data),
      tx.amount, tx.currency, tx.transaction_date, tx.transaction_time, tx.description,
      tx.merchant_name, tx.category, tx.status, tx.transaction_type, JSON.stringify(tx.source_metadata)
    ]);
  }
  
  // Publier dans Pub/Sub pour sync BigQuery
  await publishToPubSub('transactions-unified', normalized);
  
  res.json({ ingested: normalized.length });
});

// Ingestion Source B (mÃªme logique)
router.post('/ingest/source-b', requireAuth, async (req, res) => {
  // MÃªme logique que Source A mais avec normalizeSourceB
});

// Ingestion Source C
router.post('/ingest/source-c', requireAuth, async (req, res) => {
  const { events } = req.body;
  const normalized = events.map(normalizeSourceC);
  
  for (const event of normalized) {
    await pool.query(`
      INSERT INTO external_data (
        tenant_id, user_id, source_type, source_id, source_raw_data,
        event_type, event_date, event_timestamp, data_fields,
        amount, currency, description, status, priority
      ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14)
      ON CONFLICT (source_type, source_id) 
      DO UPDATE SET 
        data_fields = EXCLUDED.data_fields,
        updated_at = NOW()
    `, [
      req.user.tenantId, req.user.uid,
      event.source_type, event.source_id, JSON.stringify(event.source_raw_data),
      event.event_type, event.event_date, event.event_timestamp, JSON.stringify(event.data_fields),
      event.amount, event.currency, event.description, event.status, event.priority
    ]);
  }
  
  await publishToPubSub('external-data', normalized);
  res.json({ ingested: normalized.length });
});
```

### 3. Croisement de DonnÃ©es dans BigQuery

```sql
-- Vue BigQuery pour croiser transactions_unified et external_data
CREATE OR REPLACE VIEW `project.dataset.cross_source_analytics` AS
SELECT 
    -- DonnÃ©es transaction
    t.id as transaction_id,
    t.tenant_id,
    t.user_id,
    t.source_type as transaction_source,
    t.amount as transaction_amount,
    t.currency,
    t.transaction_date,
    t.description as transaction_description,
    t.merchant_name,
    t.category,
    
    -- DonnÃ©es external
    e.id as external_data_id,
    e.event_type,
    e.event_date,
    e.data_fields,
    e.amount as external_amount,
    e.description as external_description,
    e.priority,
    
    -- Relation
    r.relationship_type,
    r.confidence_score,
    r.match_criteria,
    
    -- Calculs croisÃ©s
    ABS(COALESCE(t.amount, 0) - COALESCE(e.amount, 0)) as amount_difference,
    DATE_DIFF(t.transaction_date, e.event_date, DAY) as date_difference_days,
    
    -- Flags
    CASE 
        WHEN t.transaction_date = e.event_date AND ABS(t.amount - COALESCE(e.amount, 0)) < 0.01 
        THEN TRUE 
        ELSE FALSE 
    END as is_exact_match,
    
    -- Timestamps
    t.ingested_at as transaction_ingested_at,
    e.ingested_at as external_ingested_at

FROM `project.dataset.transactions_unified` t
LEFT JOIN `project.dataset.data_relationships` r 
    ON t.id = r.transaction_id
LEFT JOIN `project.dataset.external_data` e 
    ON r.external_data_id = e.id
    AND t.tenant_id = e.tenant_id
    AND t.user_id = e.user_id

WHERE t.tenant_id IS NOT NULL
  AND t.transaction_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 10 YEAR); -- 10 ans de rÃ©tention
```

### 4. Analyse IA sur DonnÃ©es CroisÃ©es

```javascript
// functions/analyze-cross-data/index.js

const { BigQuery } = require('@google-cloud/bigquery');
const { VertexAI } = require('@google-cloud/vertexai');

const bigquery = new BigQuery();
const vertexAI = new VertexAI({ project: 'your-project' });

/**
 * Analyse croisÃ©e des donnÃ©es avec IA
 */
exports.analyzeCrossData = async (pubsubMessage, context) => {
  const { tenantId, userId, dateRange } = JSON.parse(
    Buffer.from(pubsubMessage.data, 'base64').toString()
  );
  
  // 1. RÃ©cupÃ©rer les donnÃ©es croisÃ©es depuis BigQuery
  const query = `
    SELECT 
      transaction_id,
      transaction_amount,
      transaction_date,
      transaction_description,
      merchant_name,
      category,
      event_type,
      event_date,
      data_fields,
      external_description,
      amount_difference,
      date_difference_days,
      is_exact_match
    FROM \`project.dataset.cross_source_analytics\`
    WHERE tenant_id = @tenantId
      AND user_id = @userId
      AND transaction_date >= @startDate
      AND transaction_date <= @endDate
    ORDER BY transaction_date DESC
    LIMIT 100
  `;
  
  const [rows] = await bigquery.query({
    query,
    params: {
      tenantId,
      userId,
      startDate: dateRange.start,
      endDate: dateRange.end,
    }
  });
  
  // 2. PrÃ©parer le contexte pour l'IA
  const contextForAI = rows.map(row => ({
    transaction: {
      amount: row.transaction_amount,
      date: row.transaction_date,
      description: row.transaction_description,
      merchant: row.merchant_name,
      category: row.category,
    },
    external: {
      type: row.event_type,
      date: row.event_date,
      description: row.external_description,
      data: row.data_fields,
    },
    relationship: {
      amountDiff: row.amount_difference,
      dateDiff: row.date_difference_days,
      isExactMatch: row.is_exact_match,
    }
  }));
  
  // 3. Analyser avec Vertex AI
  const model = vertexAI.getGenerativeModel({
    model: 'gemini-pro',
  });
  
  const prompt = `
    Analyse ces donnÃ©es croisÃ©es entre transactions bancaires et Ã©vÃ©nements externes.
    
    Pour chaque paire transaction/Ã©vÃ©nement, dÃ©termine :
    1. Le type de relation (ex: "transaction correspond Ã  facture", "doublon", "non liÃ©")
    2. Des insights (ex: "Facture payÃ©e avec 2 jours de retard", "Abonnement rÃ©current dÃ©tectÃ©")
    3. Des recommandations (ex: "Automatiser le paiement", "VÃ©rifier la facture")
    4. Un score de confiance (0-1)
    
    DonnÃ©es Ã  analyser :
    ${JSON.stringify(contextForAI, null, 2)}
    
    Retourne un JSON avec cette structure :
    {
      "analyses": [
        {
          "transaction_id": "...",
          "external_data_id": "...",
          "relationship_type": "...",
          "insights": ["..."],
          "recommendations": ["..."],
          "confidence": 0.95
        }
      ],
      "summary": {
        "total_matched": 10,
        "total_unmatched": 5,
        "key_patterns": ["..."],
        "anomalies": ["..."]
      }
    }
  `;
  
  const result = await model.generateContent(prompt);
  const analysis = JSON.parse(result.response.text());
  
  // 4. Stocker les rÃ©sultats dans BigQuery
  const analysisTable = bigquery.dataset('banking').table('ai_cross_analysis');
  
  const rowsToInsert = analysis.analyses.map(a => ({
    tenant_id: tenantId,
    user_id: userId,
    transaction_id: a.transaction_id,
    external_data_id: a.external_data_id,
    relationship_type: a.relationship_type,
    insights: a.insights,
    recommendations: a.recommendations,
    confidence: a.confidence,
    analyzed_at: new Date().toISOString(),
  }));
  
  await analysisTable.insert(rowsToInsert);
  
  // 5. Mettre Ã  jour la table de relations dans Cloud SQL
  for (const a of analysis.analyses) {
    await updateRelationshipInCloudSQL(a);
  }
  
  return { analyzed: analysis.analyses.length, summary: analysis.summary };
};
```

### 5. StratÃ©gie de RÃ©tention 10 Ans

```sql
-- Partitionnement par annÃ©e pour optimiser les requÃªtes
-- (PostgreSQL 10+ supporte le partitionnement natif)

-- Table principale (parent)
CREATE TABLE transactions_unified (
    -- ... colonnes comme dÃ©fini prÃ©cÃ©demment
) PARTITION BY RANGE (transaction_date);

-- Partitions par annÃ©e (10 ans)
CREATE TABLE transactions_unified_2024 
    PARTITION OF transactions_unified
    FOR VALUES FROM ('2024-01-01') TO ('2025-01-01');

CREATE TABLE transactions_unified_2025 
    PARTITION OF transactions_unified
    FOR VALUES FROM ('2025-01-01') TO ('2026-01-01');

-- ... crÃ©er les partitions jusqu'Ã  2034

-- Fonction pour crÃ©er automatiquement les partitions futures
CREATE OR REPLACE FUNCTION create_partition_if_not_exists(
    table_name TEXT,
    start_date DATE,
    end_date DATE
) RETURNS VOID AS $$
DECLARE
    partition_name TEXT;
BEGIN
    partition_name := table_name || '_' || TO_CHAR(start_date, 'YYYY');
    
    EXECUTE format('
        CREATE TABLE IF NOT EXISTS %I 
        PARTITION OF %I
        FOR VALUES FROM (%L) TO (%L)
    ', partition_name, table_name, start_date, end_date);
END;
$$ LANGUAGE plpgsql;

-- Script de maintenance (Ã  exÃ©cuter mensuellement)
-- CrÃ©e les partitions pour les 2 prochaines annÃ©es
DO $$
DECLARE
    year_offset INTEGER;
BEGIN
    FOR year_offset IN 0..1 LOOP
        PERFORM create_partition_if_not_exists(
            'transactions_unified',
            DATE_TRUNC('year', CURRENT_DATE + (year_offset || ' years')::INTERVAL),
            DATE_TRUNC('year', CURRENT_DATE + ((year_offset + 1) || ' years')::INTERVAL)
        );
    END LOOP;
END $$;

-- Index sur chaque partition (automatique via hÃ©ritage)
-- Les index sur la table parent s'appliquent aux partitions

-- Archivage automatique aprÃ¨s 10 ans (optionnel)
-- CrÃ©er une table d'archivage
CREATE TABLE transactions_unified_archive (
    LIKE transactions_unified INCLUDING ALL
) PARTITION BY RANGE (transaction_date);

-- Fonction d'archivage (Ã  exÃ©cuter annuellement)
CREATE OR REPLACE FUNCTION archive_old_transactions() RETURNS INTEGER AS $$
DECLARE
    archived_count INTEGER;
    cutoff_date DATE;
BEGIN
    cutoff_date := CURRENT_DATE - INTERVAL '10 years';
    
    -- DÃ©placer les donnÃ©es vers l'archive
    WITH moved AS (
        DELETE FROM transactions_unified
        WHERE transaction_date < cutoff_date
        RETURNING *
    )
    INSERT INTO transactions_unified_archive
    SELECT * FROM moved;
    
    GET DIAGNOSTICS archived_count = ROW_COUNT;
    RETURN archived_count;
END;
$$ LANGUAGE plpgsql;

-- Planifier l'archivage (via Cloud Scheduler + Cloud Function)
```

### 6. RequÃªtes de Croisement OptimisÃ©es

```sql
-- Exemple 1 : Trouver les transactions liÃ©es Ã  des Ã©vÃ©nements externes
SELECT 
    t.transaction_date,
    t.amount,
    t.description,
    e.event_type,
    e.data_fields->>'invoice_number' as invoice_number,
    r.relationship_type,
    r.confidence_score
FROM transactions_unified t
INNER JOIN data_relationships r ON t.id = r.transaction_id
INNER JOIN external_data e ON r.external_data_id = e.id
WHERE t.tenant_id = $1
  AND t.transaction_date >= CURRENT_DATE - INTERVAL '1 year'
  AND r.confidence_score > 0.8
ORDER BY t.transaction_date DESC;

-- Exemple 2 : Analyser les patterns croisÃ©s avec BigQuery
SELECT 
    t.category,
    e.event_type,
    COUNT(*) as occurrence_count,
    AVG(ABS(t.amount - COALESCE(e.amount, 0))) as avg_amount_diff,
    AVG(DATE_DIFF(t.transaction_date, e.event_date, DAY)) as avg_date_diff
FROM `project.dataset.transactions_unified` t
INNER JOIN `project.dataset.data_relationships` r ON t.id = r.transaction_id
INNER JOIN `project.dataset.external_data` e ON r.external_data_id = e.id
WHERE t.tenant_id = @tenantId
  AND t.transaction_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 1 YEAR)
GROUP BY t.category, e.event_type
ORDER BY occurrence_count DESC;
```

### 7. API pour AccÃ©der aux DonnÃ©es CroisÃ©es

```javascript
// src/routes/analytics.js

router.get('/cross-data', requireAuth, async (req, res) => {
  const { startDate, endDate, minConfidence } = req.query;
  const tenantId = req.user.tenantId;
  const userId = req.user.uid;
  
  // Option 1 : Depuis Cloud SQL (donnÃ©es rÃ©centes, < 1 an)
  if (isRecentDateRange(startDate, endDate)) {
    const result = await pool.query(`
      SELECT 
        t.*,
        e.event_type,
        e.data_fields,
        r.relationship_type,
        r.confidence_score
      FROM transactions_unified t
      LEFT JOIN data_relationships r ON t.id = r.transaction_id
      LEFT JOIN external_data e ON r.external_data_id = e.id
      WHERE t.tenant_id = $1
        AND t.user_id = $2
        AND t.transaction_date >= $3
        AND t.transaction_date <= $4
        AND (r.confidence_score IS NULL OR r.confidence_score >= $5)
      ORDER BY t.transaction_date DESC
      LIMIT 1000
    `, [tenantId, userId, startDate, endDate, minConfidence || 0]);
    
    return res.json({ data: result.rows, source: 'cloud_sql' });
  }
  
  // Option 2 : Depuis BigQuery (donnÃ©es historiques, > 1 an)
  const [rows] = await bigquery.query({
    query: `
      SELECT * FROM \`project.dataset.cross_source_analytics\`
      WHERE tenant_id = @tenantId
        AND user_id = @userId
        AND transaction_date >= @startDate
        AND transaction_date <= @endDate
        AND (confidence_score IS NULL OR confidence_score >= @minConfidence)
      ORDER BY transaction_date DESC
      LIMIT 10000
    `,
    params: { tenantId, userId, startDate, endDate, minConfidence: minConfidence || 0 }
  });
  
  res.json({ data: rows, source: 'bigquery' });
});
```

### RÃ©sumÃ© de l'Architecture Multi-Sources

âœ… **Normalisation** : ETL pipeline transforme chaque source vers un format unifiÃ©
âœ… **AgrÃ©gation** : 2 sources dans `transactions_unified`, 1 source dans `external_data`
âœ… **Croisement** : Table `data_relationships` + vue BigQuery pour analyses
âœ… **IA** : Vertex AI analyse les donnÃ©es croisÃ©es et gÃ©nÃ¨re insights
âœ… **RÃ©tention 10 ans** : Partitionnement par annÃ©e + stratÃ©gie d'archivage
âœ… **Performance** : Cloud SQL pour donnÃ©es rÃ©centes, BigQuery pour historique

---

## ğŸ“ Analyse du SchÃ©ma d'Architecture ProposÃ©

### âœ… Points Forts du SchÃ©ma

1. **SÃ©paration claire des sources**
   - Banque A & B â†’ agrÃ©gÃ©es via Tink â†’ TABLE BANCAIRES
   - PA Facture â†’ TABLE FACTURE
   - SÃ©paration logique bien pensÃ©e

2. **Tables de rÃ©sultats structurÃ©es**
   - TABLE RECONCILIEES : matching rÃ©ussi
   - TABLE INCERTAINE : matching incertain
   - Permet un traitement diffÃ©renciÃ©

3. **Utilisation de n8n pour l'ETL**
   - âœ… DÃ©jÃ  dans votre stack (mentionnÃ© dans README)
   - âœ… Interface visuelle pour workflows
   - âœ… IntÃ©grations prÃ©-construites avec Tink

4. **BigQuery pour analytics**
   - âœ… AlignÃ© avec l'architecture recommandÃ©e
   - âœ… Stockage long terme pour 10 ans

### âš ï¸ Points Ã  AmÃ©liorer

#### 1. **Architecture de l'IA : Cloud Run vs Cloud Function**

**ProblÃ¨me actuel** : L'IA semble Ãªtre dans Cloud Run (flÃ¨che bleue retour)

**Recommandation** : Utiliser **Cloud Function** ou **Vertex AI Workbench** pour l'IA

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AMÃ‰LIORATION                          â”‚
â”‚                                                          â”‚
â”‚  Cloud SQL (TABLE BANCAIRES + TABLE FACTURE)            â”‚
â”‚           â”‚                                              â”‚
â”‚           â”‚ Trigger (via Pub/Sub ou Cloud Scheduler)     â”‚
â”‚           â–¼                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚  Cloud Function (IA Matching)        â”‚              â”‚
â”‚  â”‚  - Lit les nouvelles donnÃ©es          â”‚              â”‚
â”‚  â”‚  - Appelle Vertex AI                  â”‚              â”‚
â”‚  â”‚  - Ã‰crit dans RECONCILIEES/INCERTAINEâ”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚           â”‚                                              â”‚
â”‚           â–¼                                              â”‚
â”‚  Cloud SQL (TABLE RECONCILIEES / INCERTAINE)            â”‚
â”‚           â”‚                                              â”‚
â”‚           â”‚ Trigger (via Pub/Sub)                        â”‚
â”‚           â–¼                                              â”‚
â”‚  BigQuery (pour analytics)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Pourquoi** :
- âœ… Cloud Function : dÃ©clenchement automatique, coÃ»t Ã  l'usage
- âœ… Pas besoin de maintenir un service Cloud Run dÃ©diÃ©
- âœ… ScalabilitÃ© automatique
- âœ… Meilleure isolation (si l'IA plante, Ã§a n'affecte pas l'API principale)

#### 2. **Ajout de Pub/Sub pour DÃ©couplage**

**ProblÃ¨me actuel** : Flux direct n8n â†’ Cloud Run â†’ Cloud SQL (couplage fort)

**Recommandation** : Ajouter Pub/Sub pour dÃ©coupler les composants

```
Tink â†’ n8n â†’ Pub/Sub Topic "transactions-raw"
  â†“
Cloud Function (normalize) â†’ Cloud SQL (TABLE BANCAIRES)
  â†“
Pub/Sub Topic "transactions-normalized" â†’ Cloud Function (IA Matching)
  â†“
Cloud SQL (TABLE RECONCILIEES/INCERTAINE)
  â†“
Pub/Sub Topic "reconciliation-complete" â†’ BigQuery Load
```

**Avantages** :
- âœ… RÃ©silience : si Cloud SQL est down, les messages restent en queue
- âœ… ScalabilitÃ© : plusieurs workers peuvent traiter en parallÃ¨le
- âœ… Retry automatique en cas d'erreur
- âœ… Monitoring facile (nombre de messages en queue)

#### 3. **Table de Relations Manquante**

**ProblÃ¨me actuel** : Le schÃ©ma montre seulement RECONCILIEES et INCERTAINE

**Recommandation** : Ajouter une table `data_relationships` (comme documentÃ© prÃ©cÃ©demment)

```sql
-- Table de relations (plus flexible que juste RECONCILIEES/INCERTAINE)
CREATE TABLE data_relationships (
    id UUID PRIMARY KEY,
    tenant_id UUID NOT NULL,
    transaction_id UUID REFERENCES transactions_unified(id),
    facture_id UUID REFERENCES external_data(id),
    
    -- Type de relation (plus riche que juste "reconciliÃ©e")
    relationship_type VARCHAR(50), -- 'reconciled', 'uncertain', 'partial', 'duplicate', 'related'
    confidence_score FLOAT, -- 0.0 Ã  1.0
    
    -- CritÃ¨res de matching
    match_criteria JSONB, -- {'date_diff': '2 days', 'amount_diff': 0.01, 'merchant_match': true}
    
    -- MÃ©tadonnÃ©es IA
    ai_reasoning TEXT, -- Explication de pourquoi l'IA a fait ce matching
    ai_model_version VARCHAR(50), -- Version du modÃ¨le utilisÃ©
    
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

-- Vues pour compatibilitÃ© avec votre schÃ©ma actuel
CREATE VIEW reconciled_matches AS
SELECT * FROM data_relationships 
WHERE relationship_type = 'reconciled' AND confidence_score > 0.8;

CREATE VIEW uncertain_matches AS
SELECT * FROM data_relationships 
WHERE relationship_type IN ('uncertain', 'partial') OR confidence_score <= 0.8;
```

**Avantages** :
- âœ… Plus flexible : peut gÃ©rer diffÃ©rents types de relations
- âœ… TraÃ§abilitÃ© : savoir pourquoi un matching a Ã©tÃ© fait
- âœ… Ã‰volutif : facile d'ajouter de nouveaux types de relations

#### 4. **Gestion des Erreurs et Retry**

**ProblÃ¨me actuel** : Pas de mÃ©canisme de retry visible

**Recommandation** : Ajouter une table d'audit et des dead letter queues

```sql
-- Table d'audit pour traÃ§abilitÃ©
CREATE TABLE data_ingestion_log (
    id UUID PRIMARY KEY,
    source_type VARCHAR(50), -- 'tink', 'pa_facture'
    source_id VARCHAR(255),
    status VARCHAR(20), -- 'success', 'failed', 'retry'
    error_message TEXT,
    retry_count INTEGER DEFAULT 0,
    ingested_at TIMESTAMP DEFAULT NOW(),
    processed_at TIMESTAMP
);
```

**Pub/Sub Dead Letter Topic** :
- Si une transaction Ã©choue 3 fois, elle va dans un dead letter topic
- Alertes automatiques (Cloud Monitoring)
- Interface admin pour rejouer les messages

#### 5. **Synchronisation Cloud SQL â†’ BigQuery**

**ProblÃ¨me actuel** : Le schÃ©ma montre BigQuery mais pas le mÃ©canisme de sync

**Recommandation** : Utiliser **Cloud SQL to BigQuery sync** ou **Dataflow**

```
Cloud SQL (TABLE RECONCILIEES/INCERTAINE)
  â†“
Change Data Capture (CDC) ou Pub/Sub
  â†“
Cloud Dataflow (ou Cloud Function simple)
  â†“
BigQuery (tables partitionnÃ©es)
```

**Option simple (MVP)** :
```javascript
// Cloud Function dÃ©clenchÃ©e par Pub/Sub aprÃ¨s Ã©criture dans Cloud SQL
exports.syncToBigQuery = async (pubsubMessage) => {
  const { table, record } = JSON.parse(Buffer.from(pubsubMessage.data, 'base64').toString());
  
  await bigquery
    .dataset('banking')
    .table(table)
    .insert([record]);
};
```

**Option avancÃ©e (production)** :
- Utiliser **Datastream** (GCP managed CDC) pour sync automatique
- Ou **Cloud Dataflow** pour transformations complexes

#### 6. **Feedback Loop vers PA Facture**

**ProblÃ¨me actuel** : FlÃ¨che rouge vers PA Facture mais pas claire

**Recommandation** : Clarifier le feedback loop

```
IA Matching â†’ TABLE INCERTAINE
  â†“
Cloud Function (notify)
  â†“
n8n Workflow
  â†“
PA Facture API (webhook ou email)
  â†“
Notification : "Facture 3 nÃ©cessite une vÃ©rification manuelle"
```

**Cas d'usage** :
- Factures non rÃ©conciliÃ©es aprÃ¨s X jours â†’ alerte
- Factures avec faible confiance â†’ demande de validation
- Factures dupliquÃ©es dÃ©tectÃ©es â†’ notification

### ğŸ—ï¸ Architecture AmÃ©liorÃ©e RecommandÃ©e

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SOURCES DE DONNÃ‰ES                        â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ Banque A â”‚  â”‚ Banque B â”‚         â”‚  PA Facture  â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚       â”‚            â”‚                        â”‚                â”‚
â”‚       â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                        â”‚                â”‚
â”‚             â”‚                                â”‚                â”‚
â”‚             â–¼                                â–¼                â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚      â”‚   Tink   â”‚                    â”‚   n8n    â”‚          â”‚
â”‚      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜          â”‚
â”‚            â”‚                                  â”‚                â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                       â”‚                                        â”‚
â”‚                       â–¼                                        â”‚
â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚            â”‚  Pub/Sub Topics      â”‚                           â”‚
â”‚            â”‚  - transactions-raw  â”‚                           â”‚
â”‚            â”‚  - factures-raw      â”‚                           â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                       â”‚                                        â”‚
â”‚                       â–¼                                        â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚      â”‚  Cloud Function (Normalize)       â”‚                    â”‚
â”‚      â”‚  - Normalise les formats          â”‚                    â”‚
â”‚      â”‚  - Valide les donnÃ©es             â”‚                    â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                 â”‚                                                â”‚
â”‚                 â–¼                                                â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚      â”‚         Cloud SQL                  â”‚                    â”‚
â”‚      â”‚  - TABLE BANCAIRES                 â”‚                    â”‚
â”‚      â”‚  - TABLE FACTURE                   â”‚                    â”‚
â”‚      â”‚  - data_ingestion_log (audit)      â”‚                    â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                 â”‚                                                â”‚
â”‚                 â”‚ Trigger (Pub/Sub ou Scheduler)                â”‚
â”‚                 â–¼                                                â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚      â”‚  Cloud Function (IA Matching)      â”‚                    â”‚
â”‚      â”‚  - Lit nouvelles transactions      â”‚                    â”‚
â”‚      â”‚  - Appelle Vertex AI (Gemini)     â”‚                    â”‚
â”‚      â”‚  - GÃ©nÃ¨re matching                â”‚                    â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                 â”‚                                                â”‚
â”‚                 â–¼                                                â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚      â”‚         Cloud SQL                  â”‚                    â”‚
â”‚      â”‚  - data_relationships              â”‚                    â”‚
â”‚      â”‚  - reconciled_matches (vue)         â”‚                    â”‚
â”‚      â”‚  - uncertain_matches (vue)         â”‚                    â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                 â”‚                                                â”‚
â”‚                 â”‚ Pub/Sub "reconciliation-complete"            â”‚
â”‚                 â–¼                                                â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚      â”‚  Cloud Function (Sync to BQ)       â”‚                    â”‚
â”‚      â”‚  - Ã‰crit dans BigQuery             â”‚                    â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                 â”‚                                                â”‚
â”‚                 â–¼                                                â”‚
â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                         â”‚
â”‚            â”‚ BigQuery  â”‚                                         â”‚
â”‚            â”‚ Analytics â”‚                                        â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                         â”‚
â”‚                 â”‚                                                â”‚
â”‚                 â”‚ (optionnel)                                    â”‚
â”‚                 â–¼                                                â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚      â”‚  Cloud Function (Feedback)         â”‚                    â”‚
â”‚      â”‚  - Notifie PA Facture              â”‚                    â”‚
â”‚      â”‚  - Envoie alertes                  â”‚                    â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ“‹ Checklist d'AmÃ©liorations

#### PrioritÃ© Haute (MVP)
- [ ] **Ajouter Pub/Sub** entre n8n et Cloud Run pour dÃ©couplage
- [ ] **CrÃ©er table `data_relationships`** au lieu de juste RECONCILIEES/INCERTAINE
- [ ] **DÃ©placer l'IA dans Cloud Function** plutÃ´t que Cloud Run
- [ ] **Ajouter table d'audit** `data_ingestion_log` pour traÃ§abilitÃ©

#### PrioritÃ© Moyenne (Production)
- [ ] **Mettre en place sync Cloud SQL â†’ BigQuery** automatique
- [ ] **Ajouter Dead Letter Queue** pour gestion d'erreurs
- [ ] **ImplÃ©menter feedback loop** vers PA Facture
- [ ] **Monitoring et alertes** (Cloud Monitoring)

#### PrioritÃ© Basse (Optimisation)
- [ ] **Cache Redis** pour requÃªtes frÃ©quentes
- [ ] **Partitionnement BigQuery** par date
- [ ] **Archivage automatique** aprÃ¨s 10 ans
- [ ] **Dashboard analytics** temps rÃ©el

### ğŸ¯ RÃ©sumÃ© des AmÃ©liorations

| Aspect | Ã‰tat Actuel | AmÃ©lioration RecommandÃ©e | Impact |
|--------|-------------|-------------------------|--------|
| **IA Location** | Cloud Run | Cloud Function | âœ… Meilleure scalabilitÃ©, coÃ»t rÃ©duit |
| **DÃ©couplage** | Direct | Pub/Sub | âœ… RÃ©silience, retry automatique |
| **Tables** | RECONCILIEES/INCERTAINE | + data_relationships | âœ… Plus flexible, traÃ§abilitÃ© |
| **Sync BQ** | Non spÃ©cifiÃ© | Pub/Sub + Cloud Function | âœ… Automatisation, fiabilitÃ© |
| **Gestion erreurs** | Non visible | Dead Letter Queue + Audit | âœ… ObservabilitÃ©, debugging |
| **Feedback** | FlÃ¨che vague | Workflow n8n explicite | âœ… Automatisation complÃ¨te |

---

## ğŸ¯ Conclusion

**Cloud SQL + BigQuery est une excellente architecture** pour votre cas d'usage :
- âœ… SÃ©paration claire OLTP/OLAP
- âœ… ScalabilitÃ© automatique
- âœ… IntÃ©gration native GCP
- âœ… CoÃ»ts maÃ®trisÃ©s
- âœ… PrÃªt pour l'IA (Vertex AI)

**Prochaines Ã©tapes** :
1. Commencer avec Cloud SQL pour les donnÃ©es opÃ©rationnelles
2. Mettre en place le pipeline Cloud SQL â†’ BigQuery
3. IntÃ©grer l'analyse IA progressivement
4. Monitorer les coÃ»ts et optimiser selon l'usage rÃ©el

# Diagramme d'Architecture - Flux de Donn√©es

## Vue d'ensemble du syst√®me

```mermaid
graph LR
    %% Styles
    classDef external fill:#f9f,stroke:#333,stroke-width:2px;
    classDef ingestion fill:#ffcc00,stroke:#333,stroke-width:2px;
    classDef storage fill:#3399ff,stroke:#333,stroke-width:2px,color:white;
    classDef process fill:#9933ff,stroke:#333,stroke-width:2px,color:white;
    classDef userzone fill:#00cc66,stroke:#333,stroke-width:2px,color:white;

    %% Acteurs Externes
    subgraph External["üåç Sources Externes"]
        Tink[("üè¶ Tink (Banques)")]:::external
        ERP[("üìä ERP\n(Factures)")]:::external
        Webhooks[("üì® Webhooks\n(Providers)")]:::external
        ExternalClients[("üîå API Clients\n(CSV/JSON Upload)")]:::external
    end

    %% ZONE 1 : INGESTION (Le Bouclier)
    subgraph ZoneIngestion["üõ°Ô∏è Zone Ingestion (Async)"]
        n8n["üêô n8n\n(Orchestrator)"]:::ingestion
        CoreWebhook["üîî Cloud Run\n(Service: Core Webhook)"]:::ingestion
        PubSubIngest[("üì® Google Pub/Sub\n(Topic: ingest)")]:::ingestion
        IngestWorker["‚öôÔ∏è Cloud Run\n(Service: Ingest Worker)"]:::ingestion
    end

    %% ZONE 2 : DATA (Le Coeur)
    subgraph ZoneData["üíæ Zone Donn√©es (Master)"]
        CloudSQL[("üõ¢Ô∏è Cloud SQL\n(PostgreSQL)\nMultiple Tables")]:::storage
    end

    %% ZONE 3 : TRAITEMENT (Le Cerveau)
    subgraph ZoneProcess["üß† Zone Traitement (Worker)"]
        ProcessWorker["‚öôÔ∏è Cloud Run\n(Service: Worker/Jobs)"]:::process
        IAMatch["ü§ñ IA Matching\n(Vertex AI)"]:::process
        PubSubProcess[("üì® Google Pub/Sub\n(Topic: process)")]:::process
    end

    %% ZONE 4 : UTILISATEUR (La Fa√ßade)
    subgraph ZoneUser["üë§ Zone Utilisateur (API-First)"]
        CoreAPI["üöÄ Cloud Run\n(Service: Core API)\nREST/GraphQL"]:::userzone
        Frontend["üíª Frontend\n(Lovable/React)"]:::userzone
    end

    %% FLUX DE DONNEES
    %% Flux Banques via n8n
    Tink -- "1. JSON Data" --> n8n
    
    %% Flux ERP Factures via n8n
    ERP -- "2. Factures Data" --> n8n
    
    %% Consolidation n8n vers Pub/Sub
    n8n -- "3. Push Message" --> PubSubIngest
    
    %% Flux Webhooks
    Webhooks -- "Webhook Events" --> CoreWebhook
    CoreWebhook -- "Validate & Push" --> PubSubIngest
    
    %% Flux Ingestion vers Data
    PubSubIngest -- "4. D√©pile (Batch)" --> IngestWorker
    IngestWorker -- "5. INSERT (Bulk)" --> CloudSQL

    %% Flux Traitement (apr√®s ingestion compl√®te)
    CloudSQL -- "6. Trigger\n(Data Ready)" --> PubSubProcess
    PubSubProcess -- "7. D√©pile" --> ProcessWorker
    ProcessWorker -- "8. Lit (Unmatched)" --> CloudSQL
    ProcessWorker -- "9. Appelle IA" --> IAMatch
    IAMatch -- "10. R√©sultats" --> ProcessWorker
    ProcessWorker -- "11. UPDATE (Matches)" --> CloudSQL

    %% Flux Emails/Alertes
    CloudSQL -- "Events" --> CoreAPI
    CoreAPI -- "Read Alerts Queue" --> CloudSQL

    %% Flux Upload CSV/JSON
    ExternalClients -- "Upload CSV/JSON" --> CoreAPI
    CoreAPI -- "Parse & Validate" --> PubSubIngest
    
    %% Flux Utilisateur
    Frontend -- "HTTPS / GraphQL" --> CoreAPI
    ExternalClients -- "API Calls" --> CoreAPI
    CoreAPI -- "SELECT (Read)" --> CloudSQL
    CoreAPI -- "CRUD Operations" --> CloudSQL
```

## Micro-services Cloud Run

- **Core API** : API principale **API-First** (REST/GraphQL) pour :
  - Requ√™tes utilisateur (CRUD)
  - Upload de fichiers CSV/JSON
  - Gestion des emails/alertes
  - Point d'entr√©e pour tous les clients (Frontend + API externes)
- **Core Webhook** : Service d√©di√© √† la r√©ception et validation des webhooks externes
- **Ingest Worker** : Service d'ingestion qui √©crit les donn√©es dans Cloud SQL (toutes sources confondues)
- **Worker/Jobs** : Service de traitement qui orchestre l'IA apr√®s ingestion compl√®te (√† venir)

## L√©gende des Zones

- **üõ°Ô∏è Zone Ingestion (Async)** : Point d'entr√©e asynchrone pour toutes les donn√©es externes (n8n, Core Webhook, Ingest Worker)
- **üíæ Zone Donn√©es (Master)** : Source de v√©rit√©, stockage principal des donn√©es (Cloud SQL PostgreSQL avec plusieurs tables - voir ci-dessous)
- **üß† Zone Traitement (Worker)** : Traitement asynchrone des donn√©es - L'IA intervient UNIQUEMENT apr√®s que toutes les donn√©es soient dans Cloud SQL (√† venir)
- **üë§ Zone Utilisateur (API-First)** : API principale (Core API) accessible par le Frontend ET par des clients externes + gestion des emails/alertes

## Tables principales dans Cloud SQL

Cloud SQL (PostgreSQL) contient plusieurs tables organis√©es par domaine :

### Tables Bancaires
- **bank_connections** : Connexions aux providers bancaires (Tink, etc.)
- **bank_accounts** : Comptes bancaires li√©s aux connexions
- **transactions** : Transactions bancaires (ou `transactions_unified` selon l'architecture)

### Tables Factures/ERP
- **invoices** / **factures** : Factures provenant de l'ERP
- **external_data** : Donn√©es externes (si architecture multi-sources)

### Tables Relations/Matching
- **data_relationships** : Relations entre transactions et factures (matching IA)
- **reconciliation_matches** : R√©sultats de r√©conciliation

### Tables Syst√®me
- **emails_queue** / **alerts_queue** : Queue d'emails et alertes √† envoyer
- **data_ingestion_log** : Logs d'ingestion pour tra√ßabilit√©
- **import_jobs** : Suivi des imports CSV/JSON (statut, erreurs, etc.)
- **users**, **tenants** : Gestion des utilisateurs et organisations (si n√©cessaire)

## Types de Composants

- **Sources Externes** (Rose) : Sources de donn√©es externes (Tink, ERP, Webhooks providers)
- **Ingestion** (Jaune) : Services d'ingestion et orchestration (n8n, Core Webhook, Ingest Worker, Pub/Sub)
- **Stockage** (Bleu) : Base de donn√©es principale (Cloud SQL PostgreSQL - plusieurs tables pour transactions, factures, connexions, emails/alertes, etc.)
- **Traitement** (Violet) : Services de traitement et IA (Worker/Jobs, Vertex AI, Pub/Sub) - √† venir
- **Utilisateur** (Vert) : API et interface utilisateur (Core API, Frontend)

## Flux de Donn√©es

1. **Ingestion Banques** : Tink ‚Üí n8n ‚Üí Pub/Sub ‚Üí Ingest Worker ‚Üí Cloud SQL
2. **Ingestion Factures ERP** : ERP ‚Üí n8n ‚Üí Pub/Sub ‚Üí Ingest Worker ‚Üí Cloud SQL
3. **Webhooks** : Providers ‚Üí Core Webhook ‚Üí Pub/Sub ‚Üí Ingest Worker ‚Üí Cloud SQL
4. **Upload CSV/JSON** : Clients externes ‚Üí Core API (upload/parse) ‚Üí Pub/Sub ‚Üí Ingest Worker ‚Üí Cloud SQL
5. **Traitement IA** : Cloud SQL (toutes donn√©es ing√©r√©es) ‚Üí Pub/Sub ‚Üí Worker/Jobs ‚Üí Vertex AI ‚Üí Cloud SQL (r√©sultats) - √† venir
6. **Emails/Alertes** : Cloud SQL (queue d'alertes) ‚Üí Core API ‚Üí Envoi emails/notifications
7. **Lecture/√âcriture Utilisateur** : Frontend/API Clients ‚Üí Core API ‚Üí Cloud SQL (CRUD)

## √Ä propos des Webhooks

**√Ä quoi servent les Webhook Events ?**

Les webhooks sont des notifications en temps r√©el envoy√©es par des providers externes (comme Tink, ou d'autres services) pour signaler des √©v√©nements importants :

- **Nouvelles transactions** : Tink peut envoyer un webhook quand une nouvelle transaction appara√Æt sur un compte connect√©
- **Changements de statut** : Notifications de changements (compte d√©connect√©, token expir√©, etc.)
- **√âv√©nements m√©tier** : D'autres services peuvent notifier des √©v√©nements importants pour votre syst√®me

Le service **Core Webhook** re√ßoit ces webhooks, les valide (s√©curit√©, authentification), puis les envoie dans Pub/Sub pour traitement asynchrone par l'Ingest Worker. Cela permet une r√©action en temps r√©el aux √©v√©nements externes sans polling continu.

---

## Architecture API-First

### Principe
L'application suit une architecture **API-First** : **Core API** est le point d'entr√©e unique pour tous les clients (Frontend, clients externes, int√©grations).

### Avantages
- ‚úÖ **R√©utilisabilit√©** : L'API peut √™tre utilis√©e par diff√©rents clients (web, mobile, int√©grations)
- ‚úÖ **D√©couplage** : Le Frontend et les autres clients sont ind√©pendants
- ‚úÖ **√âvolutivit√©** : Facile d'ajouter de nouveaux clients sans modifier le backend
- ‚úÖ **Documentation** : API document√©e (OpenAPI/Swagger) pour faciliter l'int√©gration

### Endpoints Principaux de Core API

#### 1. CRUD Standard
- `GET /api/transactions` - Liste des transactions
- `GET /api/transactions/:id` - D√©tail d'une transaction
- `POST /api/transactions` - Cr√©er une transaction
- `PUT /api/transactions/:id` - Mettre √† jour
- `DELETE /api/transactions/:id` - Supprimer

#### 2. Upload de Fichiers CSV/JSON
- `POST /api/ingest/upload` - Upload CSV/JSON
  - Accepte : `multipart/form-data` (fichier) ou JSON (donn√©es directes)
  - Formats support√©s : CSV, JSON
  - Validation automatique
  - Parsing et normalisation
  - Envoi dans Pub/Sub pour traitement asynchrone

#### 3. Export de Donn√©es
- `GET /api/export/transactions` - Export CSV/JSON
- `GET /api/export/factures` - Export factures

### Authentification
- **Firebase Auth** pour le Frontend (JWT tokens)
- **API Keys** ou **OAuth2** pour les clients externes
- Chaque requ√™te authentifi√©e avec `tenant_id` pour isolation multi-tenant

---

## Import CSV/JSON - Impl√©mentation

### Flow d'Import

1. **Upload** : Client envoie fichier CSV/JSON √† `POST /api/ingest/upload`
2. **Validation** : Core API valide le format et le sch√©ma
3. **Parsing** : Extraction et normalisation des donn√©es
4. **Queue** : Envoi dans Pub/Sub (Topic: ingest)
5. **Traitement** : Ingest Worker traite de mani√®re asynchrone
6. **Stockage** : √âcriture dans Cloud SQL
7. **R√©ponse** : Statut de l'upload retourn√© au client

### Format CSV Attendu

```csv
date,amount,description,category,merchant
2024-01-15,125.50,Restaurant Le Paris,category_restaurant,Le Paris
2024-01-16,29.99,Netflix Subscription,category_subscription,Netflix
```

### Format JSON Attendu

```json
{
  "transactions": [
    {
      "date": "2024-01-15",
      "amount": 125.50,
      "currency": "EUR",
      "description": "Restaurant Le Paris",
      "category": "category_restaurant",
      "merchant": "Le Paris"
    }
  ]
}
```

### Modifications Techniques N√©cessaires

#### 1. Core API - Nouveau Endpoint
```javascript
// src/routes/ingest.js
router.post('/upload', requireAuth, upload.single('file'), async (req, res) => {
  const file = req.file; // CSV ou JSON
  const tenantId = req.user.tenantId;
  
  // Parse le fichier
  const data = await parseFile(file);
  
  // Valider le sch√©ma
  const validated = await validateData(data);
  
  // Publier dans Pub/Sub
  await publishToPubSub('ingest', {
    tenantId,
    source: 'api_upload',
    data: validated,
    uploadedBy: req.user.uid
  });
  
  res.json({ 
    status: 'queued',
    recordsCount: validated.length,
    jobId: jobId
  });
});
```

#### 2. Service de Parsing
- **CSV** : Utiliser `csv-parse` ou `papaparse`
- **JSON** : Parsing natif + validation JSON Schema
- **Normalisation** : Adapter au format interne

#### 3. Validation
- **JSON Schema** pour valider la structure
- **Business Rules** (montants valides, dates, etc.)
- **Limites** : Taille max fichier, nombre de lignes

#### 4. Monitoring
- Table `import_jobs` dans Cloud SQL pour tracker les imports
- Statut : `pending`, `processing`, `completed`, `failed`
- Logs d'erreurs d√©taill√©s

### Tables N√©cessaires (ajout)

```sql
-- Table pour tracker les imports
CREATE TABLE import_jobs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    tenant_id UUID NOT NULL,
    user_id UUID NOT NULL,
    source_type VARCHAR(50) DEFAULT 'api_upload',
    file_name VARCHAR(255),
    file_type VARCHAR(10), -- 'csv', 'json'
    status VARCHAR(20) DEFAULT 'pending', -- 'pending', 'processing', 'completed', 'failed'
    records_count INTEGER,
    records_processed INTEGER DEFAULT 0,
    errors JSONB,
    created_at TIMESTAMP DEFAULT NOW(),
    completed_at TIMESTAMP
);

CREATE INDEX idx_import_jobs_tenant ON import_jobs(tenant_id);
CREATE INDEX idx_import_jobs_status ON import_jobs(status);
```
